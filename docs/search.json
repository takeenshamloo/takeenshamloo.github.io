[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blogs",
    "section": "",
    "text": "EDS 240: Homework #4\n\n\n\n\n\n\nTakeen Shamloo\n\n\n\n\n\n\n\n\n\n\n\n\nBiodiversity Intactness Analysis for Phoenix Subdivision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing and Understanding the Thomas Fire\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreenhouse Gas Emissions Analysis\n\n\n\n\n\n\nTakeen Shamloo\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/final_project_template.html",
    "href": "posts/final_project_template.html",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "",
    "text": "Introduction:\nGreenhouse gases are a major driver of climate change, affecting everything from our air quality to the economy’s stability. In the United States, the energy industry is a major contributor to GHG emissions because of how we use coal, natural gas, and oil. It’s really important to understand the impact of these different fuel sources so we can come up with better climate policies and move towards cleaner energy in the future.\nThis study asks an important question: How much do emissions from coal and natural gas predict the total amount of GHG emissions in the U.S.? By looking at emissions data from the past fifty years, we hope to learn more about emission trends and how we can make better energy policies to reduce them.\nThis research is important because it helps connect what we know about emissions from a scientific standpoint to how we actually implement policies. While a lot of existing studies focus on overall emissions, there’s not as much detail about the specific contributions of coal and natural gas. This study aims to fill that gap and give us a clearer picture of how these fuel sources impact our environment and how the energy sector is changing over time.\n\n\nThe dataset used in this analysis is available from the U.S. Energy Information Administration (EIA). Link to the data.\nMotivation: Understanding the role of coal and natural gas emissions in contributing to total GHG emissions is essential for shaping energy policies. These insights can help evaluate the effectiveness of transitioning to cleaner energy sources and inform future environmental strategies. Prior research has focused on aggregate emissions trends, but the specific contributions of individual fuel sources require further exploration.\nHypotheses:\n\nNull Hypothesis (H₀): Coal and natural gas emissions do not significantly predict total GHG emissions.\nAlternative Hypothesis (H₁): Coal and natural gas emissions significantly predict total GHG emissions."
  },
  {
    "objectID": "posts/final_project_template.html#data-availability",
    "href": "posts/final_project_template.html#data-availability",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "",
    "text": "The dataset used in this analysis is available from the U.S. Energy Information Administration (EIA). Link to the data.\nMotivation: Understanding the role of coal and natural gas emissions in contributing to total GHG emissions is essential for shaping energy policies. These insights can help evaluate the effectiveness of transitioning to cleaner energy sources and inform future environmental strategies. Prior research has focused on aggregate emissions trends, but the specific contributions of individual fuel sources require further exploration.\nHypotheses:\n\nNull Hypothesis (H₀): Coal and natural gas emissions do not significantly predict total GHG emissions.\nAlternative Hypothesis (H₁): Coal and natural gas emissions significantly predict total GHG emissions."
  },
  {
    "objectID": "posts/final_project_template.html#load-and-clean-data",
    "href": "posts/final_project_template.html#load-and-clean-data",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Load and Clean Data",
    "text": "Load and Clean Data\n\nannual_data &lt;- read_excel(\n  here(\"data\", \"Table_11.1_Carbon_Dioxide_Emissions_From_Energy_Consumption_by_Source.xlsx\"),\n  sheet = \"Annual Data\", \n  skip = 10\n) %&gt;%\n  clean_names() %&gt;% \n  filter(!is.na(annual_total)) %&gt;%\n  rename(year = annual_total) %&gt;%\n  mutate(across(-year, as.numeric))\n\n# Preview the cleaned dataset\nglimpse(annual_data)\n\nRows: 51\nColumns: 15\n$ year                                                           &lt;dbl&gt; 1973, 1…\n$ coal_including_coal_coke_net_imports_co2_emissions             &lt;dbl&gt; 1220.71…\n$ natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions &lt;dbl&gt; 1175.25…\n$ aviation_gasoline_co2_emissions                                &lt;dbl&gt; 5.771, …\n$ distillate_fuel_oil_excluding_biodiesel_co2_emissions          &lt;dbl&gt; 485.041…\n$ hydrocarbon_gas_liquids_co2_emissions                          &lt;dbl&gt; 79.540,…\n$ jet_fuel_co2_emissions                                         &lt;dbl&gt; 154.162…\n$ kerosene_co2_emissions                                         &lt;dbl&gt; 32.747,…\n$ lubricants_co2_emissions                                       &lt;dbl&gt; 13.290,…\n$ motor_gasoline_excluding_ethanol_co2_emissions                 &lt;dbl&gt; 911.241…\n$ petroleum_coke_co2_emissions                                   &lt;dbl&gt; 55.137,…\n$ residual_fuel_oil_co2_emissions                                &lt;dbl&gt; 486.200…\n$ other_petroleum_products_co2_emissions                         &lt;dbl&gt; 102.153…\n$ petroleum_excluding_biofuels_co2_emissions                     &lt;dbl&gt; 2325.28…\n$ total_energy_co2_emissions                                     &lt;dbl&gt; 4721.24…\n\n\nData Description:\n\nSource: U.S. Energy Information Administration (EIA)\nTemporal Range: 1973–2023\nVariables: Includes emissions from coal, natural gas, and petroleum, along with total GHG emissions.\nLimitations: Sampling does not account for smaller fuel sources or regional variations. Emissions data rely on aggregate national estimates, which could introduce bias.\n\nVisualizing the dataset reveals important temporal trends. The summary statistics underscore the significant contributions of coal and natural gas to total emissions. However, the absence of regional granularity introduces challenges, as emissions vary geographically and temporally based on policy and technological advancements."
  },
  {
    "objectID": "posts/final_project_template.html#exploratory-data-analysis",
    "href": "posts/final_project_template.html#exploratory-data-analysis",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n# Summary statistics\nsummary(annual_data)\n\n      year      coal_including_coal_coke_net_imports_co2_emissions\n Min.   :1973   Min.   : 777.3                                    \n 1st Qu.:1986   1st Qu.:1336.6                                    \n Median :1998   Median :1713.4                                    \n Mean   :1998   Mean   :1658.1                                    \n 3rd Qu.:2010   3rd Qu.:1990.2                                    \n Max.   :2023   Max.   :2180.4                                    \n natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions\n Min.   : 868.2                                                \n 1st Qu.:1046.6                                                \n Median :1191.4                                                \n Mean   :1218.6                                                \n 3rd Qu.:1302.4                                                \n Max.   :1759.7                                                \n aviation_gasoline_co2_emissions\n Min.   :1.397                  \n 1st Qu.:1.853                  \n Median :2.583                  \n Mean   :2.794                  \n 3rd Qu.:3.294                  \n Max.   :5.771                  \n distillate_fuel_oil_excluding_biodiesel_co2_emissions\n Min.   :418.9                                        \n 1st Qu.:476.3                                        \n Median :543.3                                        \n Mean   :542.8                                        \n 3rd Qu.:604.1                                        \n Max.   :657.6                                        \n hydrocarbon_gas_liquids_co2_emissions jet_fuel_co2_emissions\n Min.   : 71.19                        Min.   :144.1         \n 1st Qu.: 79.41                        1st Qu.:174.7         \n Median : 85.68                        Median :215.2         \n Mean   : 86.80                        Mean   :209.2         \n 3rd Qu.: 95.21                        3rd Qu.:241.6         \n Max.   :111.37                        Max.   :260.6         \n kerosene_co2_emissions lubricants_co2_emissions\n Min.   : 0.784         Min.   : 6.806          \n 1st Qu.: 2.007         1st Qu.:10.742          \n Median : 8.274         Median :12.000          \n Mean   :10.599         Mean   :11.687          \n 3rd Qu.:16.065         3rd Qu.:12.914          \n Max.   :32.747         Max.   :14.717          \n motor_gasoline_excluding_ethanol_co2_emissions petroleum_coke_co2_emissions\n Min.   : 892.5                                 Min.   : 48.33              \n 1st Qu.: 978.6                                 1st Qu.: 56.55              \n Median :1065.0                                 Median : 72.71              \n Mean   :1047.7                                 Mean   : 72.74              \n 3rd Qu.:1127.1                                 3rd Qu.: 81.08              \n Max.   :1216.9                                 Max.   :111.67              \n residual_fuel_oil_co2_emissions other_petroleum_products_co2_emissions\n Min.   : 35.87                  Min.   : 73.79                        \n 1st Qu.: 83.63                  1st Qu.:110.39                        \n Median :149.39                  Median :118.06                        \n Mean   :193.60                  Mean   :117.05                        \n 3rd Qu.:237.29                  3rd Qu.:129.19                        \n Max.   :529.04                  Max.   :150.88                        \n petroleum_excluding_biofuels_co2_emissions total_energy_co2_emissions\n Min.   :1978                               Min.   :4384              \n 1st Qu.:2193                               1st Qu.:4786              \n Median :2255                               Median :5147              \n Mean   :2295                               Mean   :5179              \n 3rd Qu.:2398                               3rd Qu.:5554              \n Max.   :2633                               Max.   :6015              \n\n# Trends in total emissions\nannual_data %&gt;%\n  ggplot(aes(x = year, y = total_energy_co2_emissions)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"loess\", color = \"red\", size = 0.8) +\n  labs(\n    title = \"Trend in Total CO2 Emissions (1973-2023)\",\n    x = \"Year\",\n    y = \"Total CO2 Emissions (Million Metric Tons)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFindings:\nThe graph shows emissions steadily rising until 2005, peaking at about 6000 million metric tons. This is followed by a gradual decline, aligning with shifts in energy reliance driven by climate policies and technological innovation. The adoption of renewable energy sources like wind and solar played a crucial role in reversing this trend.\nThe shift around 2005 is particularly significant, as it coincides with major federal and state-level environmental policies aimed at reducing coal use, such as the Clean Air Interstate Rule (CAIR). These efforts, coupled with the economic viability of natural gas due to advancements in hydraulic fracturing, marked a turning point in the U.S. energy sector."
  },
  {
    "objectID": "posts/final_project_template.html#emissions-by-fuel-type",
    "href": "posts/final_project_template.html#emissions-by-fuel-type",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Emissions by Fuel Type",
    "text": "Emissions by Fuel Type\n\n# Bar plot for emissions by fuel type\nannual_data %&gt;%\n  pivot_longer(cols = c(\n    coal_including_coal_coke_net_imports_co2_emissions,\n    natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions,\n    petroleum_excluding_biofuels_co2_emissions\n  ), \n  names_to = \"fuel_type\", \n  values_to = \"emissions\") %&gt;%\n  mutate(fuel_type = case_when(\n    fuel_type == \"coal_including_coal_coke_net_imports_co2_emissions\" ~ \"Coal\",\n    fuel_type == \"natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions\" ~ \"Natural Gas\",\n    fuel_type == \"petroleum_excluding_biofuels_co2_emissions\" ~ \"Petroleum\",\n    TRUE ~ fuel_type\n  )) %&gt;%\n  ggplot(aes(x = fuel_type, y = emissions, fill = as.factor(year))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"CO2 Emissions by Fuel Type (1973-2023)\",\n    x = \"Fuel Type\",\n    y = \"Emissions (Million Metric Tons)\",\n    fill = \"Year\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFindings:\nThe bar plot underscores a sharp decline in coal emissions post-2005, coinciding with increased adoption of natural gas. Despite stable petroleum emissions, the energy landscape has undergone significant transformations, signaling policy-driven shifts and advancements in energy efficiency.\nNatural gas stands out as a ‘bridge fuel.’ Its cleaner than coal, making it a popular choice. But there’s a downside: leaks during production and transport could undo its benefits. This underscores the need for balanced policy approaches that weigh short-term reductions against long-term sustainability.\nAdditionally, the stability of petroleum emissions indicates that the transportation sector remains a significant challenge. The lack of substantial reduction in this area highlights the need for increased adoption of electric vehicles and alternative fuels."
  },
  {
    "objectID": "posts/final_project_template.html#model-setup",
    "href": "posts/final_project_template.html#model-setup",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Model Setup",
    "text": "Model Setup\n\\[\n\\text{Total CO}_2 \\text{ Emissions} = \\beta_0 + \\beta_1(\\text{Year}) + \\beta_2(\\text{Coal Emissions}) + \\beta_3(\\text{Natural Gas Emissions})\n\\]\n\n# Fit a linear model\nlm_model &lt;- lm(\n  total_energy_co2_emissions ~ year + coal_including_coal_coke_net_imports_co2_emissions + natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions,\n  data = annual_data\n)\n\n# Model summary\nsummary(lm_model)\n\n\nCall:\nlm(formula = total_energy_co2_emissions ~ year + coal_including_coal_coke_net_imports_co2_emissions + \n    natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions, \n    data = annual_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-183.90 -106.54  -11.34   65.39  324.11 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     2.404e+04\nyear                                                           -1.191e+01\ncoal_including_coal_coke_net_imports_co2_emissions              1.430e+00\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions  2.110e+00\n                                                               Std. Error\n(Intercept)                                                     6.593e+03\nyear                                                            3.484e+00\ncoal_including_coal_coke_net_imports_co2_emissions              7.308e-02\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions  2.480e-01\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      3.647 0.000664\nyear                                                            -3.420 0.001306\ncoal_including_coal_coke_net_imports_co2_emissions              19.570  &lt; 2e-16\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions   8.510 4.47e-11\n                                                                  \n(Intercept)                                                    ***\nyear                                                           ** \ncoal_including_coal_coke_net_imports_co2_emissions             ***\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 126.9 on 47 degrees of freedom\nMultiple R-squared:  0.9325,    Adjusted R-squared:  0.9282 \nF-statistic: 216.5 on 3 and 47 DF,  p-value: &lt; 2.2e-16\n\n\nFindings:\nRegression analysis reveals:\n\nCoal and Natural Gas as Key Predictors: Coal (β = 1.43) and natural gas (β = 2.11) emissions are significant predictors of total emissions. These coefficients underscore their critical role in shaping national emissions profiles.\nTemporal Decline: The year variable (β = -11.91) reflects a downward trend, aligning with broader shifts toward decarbonization.\nModel Strength: An adjusted R² of 0.93 indicates the model effectively explains 93% of total emissions variation.\n\nThe significant coefficients validate the importance of coal and natural gas in determining overall GHG emissions. The results highlight that while natural gas is contributing to reductions in total emissions, its overall impact is moderated by its role in replacing coal rather than being phased out itself."
  },
  {
    "objectID": "posts/final_project_template.html#model-diagnostics",
    "href": "posts/final_project_template.html#model-diagnostics",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\n# Durbin-Watson Test\nlibrary(lmtest)\ndwtest(lm_model)\n\n\n    Durbin-Watson test\n\ndata:  lm_model\nDW = 0.46072, p-value = 9.964e-14\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Residual plots\nplot(lm_model, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFindings:\nThe residual diagnostics point to positive autocorrelation (DW = 0.46, p &lt; 0.001). In simpler terms, the data suggests that we need more sophisticated models, like ARIMA, to make better predictions.\nPositive autocorrelation in the residuals suggests that omitted variables or structural shifts in energy markets may influence the model. Addressing these factors would enhance the predictive capacity of future analyses."
  },
  {
    "objectID": "posts/final_project_template.html#arima-modeling",
    "href": "posts/final_project_template.html#arima-modeling",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "ARIMA Modeling",
    "text": "ARIMA Modeling\n\n# Fit ARIMA model\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nemissions_ts &lt;- ts(annual_data$total_energy_co2_emissions, start = min(annual_data$year), frequency = 1)\narima_model &lt;- auto.arima(emissions_ts)\n\n# Forecast\nforecast(arima_model, h = 10) %&gt;%\n  autoplot() +\n  labs(\n    title = \"ARIMA Forecast for Total CO2 Emissions\",\n    x = \"Year\",\n    y = \"Total CO2 Emissions (Million Metric Tons)\"\n  )\n\n\n\n\n\n\n\n\nFindings:\nThe ARIMA forecast suggests that the emissions will continue to drop at a slow rate, but with increasing variability of the predictions over time. This is consistent with the importance of the efforts for the decarbonization, which emphasise the importance of policies in the process.\nThis shows the limitations of depending on past tendencies, for example, external events such as technological advancements, changes in the economy or policies across the globe may affect these predictions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/ghg_stats_final.html",
    "href": "posts/ghg_stats_final.html",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "",
    "text": "Introduction:\nGreenhouse gases are a major driver of climate change, affecting everything from our air quality to the economy’s stability. In the United States, the energy industry is a major contributor to GHG emissions because of how we use coal, natural gas, and oil. It’s really important to understand the impact of these different fuel sources so we can come up with better climate policies and move towards cleaner energy in the future.\nThis study asks an important question: How much do emissions from coal and natural gas predict the total amount of GHG emissions in the U.S.? By looking at emissions data from the past fifty years, we hope to learn more about emission trends and how we can make better energy policies to reduce them.\nThis research is important because it helps connect what we know about emissions from a scientific standpoint to how we actually implement policies. While a lot of existing studies focus on overall emissions, there’s not as much detail about the specific contributions of coal and natural gas. This study aims to fill that gap and give us a clearer picture of how these fuel sources impact our environment and how the energy sector is changing over time.\n\n\nThe dataset used in this analysis is available from the U.S. Energy Information Administration (EIA). Link to the data.\nMotivation: Understanding the role of coal and natural gas emissions in contributing to total GHG emissions is essential for shaping energy policies. These insights can help evaluate the effectiveness of transitioning to cleaner energy sources and inform future environmental strategies. Prior research has focused on aggregate emissions trends, but the specific contributions of individual fuel sources require further exploration.\nHypotheses:\n\nNull Hypothesis (H₀): Coal and natural gas emissions do not significantly predict total GHG emissions.\nAlternative Hypothesis (H₁): Coal and natural gas emissions significantly predict total GHG emissions."
  },
  {
    "objectID": "posts/ghg_stats_final.html#data-availability",
    "href": "posts/ghg_stats_final.html#data-availability",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "",
    "text": "The dataset used in this analysis is available from the U.S. Energy Information Administration (EIA). Link to the data.\nMotivation: Understanding the role of coal and natural gas emissions in contributing to total GHG emissions is essential for shaping energy policies. These insights can help evaluate the effectiveness of transitioning to cleaner energy sources and inform future environmental strategies. Prior research has focused on aggregate emissions trends, but the specific contributions of individual fuel sources require further exploration.\nHypotheses:\n\nNull Hypothesis (H₀): Coal and natural gas emissions do not significantly predict total GHG emissions.\nAlternative Hypothesis (H₁): Coal and natural gas emissions significantly predict total GHG emissions."
  },
  {
    "objectID": "posts/ghg_stats_final.html#load-and-clean-data",
    "href": "posts/ghg_stats_final.html#load-and-clean-data",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Load and Clean Data",
    "text": "Load and Clean Data\n\nannual_data &lt;- read_excel(\n  here(\"data\", \"Table_11.1_Carbon_Dioxide_Emissions_From_Energy_Consumption_by_Source.xlsx\"),\n  sheet = \"Annual Data\", \n  skip = 10\n) %&gt;%\n  clean_names() %&gt;% \n  filter(!is.na(annual_total)) %&gt;%\n  rename(year = annual_total) %&gt;%\n  mutate(across(-year, as.numeric))\n\n# Preview the cleaned dataset\nglimpse(annual_data)\n\nRows: 51\nColumns: 15\n$ year                                                           &lt;dbl&gt; 1973, 1…\n$ coal_including_coal_coke_net_imports_co2_emissions             &lt;dbl&gt; 1220.71…\n$ natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions &lt;dbl&gt; 1175.25…\n$ aviation_gasoline_co2_emissions                                &lt;dbl&gt; 5.771, …\n$ distillate_fuel_oil_excluding_biodiesel_co2_emissions          &lt;dbl&gt; 485.041…\n$ hydrocarbon_gas_liquids_co2_emissions                          &lt;dbl&gt; 79.540,…\n$ jet_fuel_co2_emissions                                         &lt;dbl&gt; 154.162…\n$ kerosene_co2_emissions                                         &lt;dbl&gt; 32.747,…\n$ lubricants_co2_emissions                                       &lt;dbl&gt; 13.290,…\n$ motor_gasoline_excluding_ethanol_co2_emissions                 &lt;dbl&gt; 911.241…\n$ petroleum_coke_co2_emissions                                   &lt;dbl&gt; 55.137,…\n$ residual_fuel_oil_co2_emissions                                &lt;dbl&gt; 486.200…\n$ other_petroleum_products_co2_emissions                         &lt;dbl&gt; 102.153…\n$ petroleum_excluding_biofuels_co2_emissions                     &lt;dbl&gt; 2325.28…\n$ total_energy_co2_emissions                                     &lt;dbl&gt; 4721.24…\n\n\nData Description:\n\nSource: U.S. Energy Information Administration (EIA)\nTemporal Range: 1973–2023\nVariables: Includes emissions from coal, natural gas, and petroleum, along with total GHG emissions.\nLimitations: Sampling does not account for smaller fuel sources or regional variations. Emissions data rely on aggregate national estimates, which could introduce bias.\n\nVisualizing the dataset reveals important temporal trends. The summary statistics underscore the significant contributions of coal and natural gas to total emissions. However, the absence of regional granularity introduces challenges, as emissions vary geographically and temporally based on policy and technological advancements."
  },
  {
    "objectID": "posts/ghg_stats_final.html#exploratory-data-analysis",
    "href": "posts/ghg_stats_final.html#exploratory-data-analysis",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n# Summary statistics\nsummary(annual_data)\n\n      year      coal_including_coal_coke_net_imports_co2_emissions\n Min.   :1973   Min.   : 777.3                                    \n 1st Qu.:1986   1st Qu.:1336.6                                    \n Median :1998   Median :1713.4                                    \n Mean   :1998   Mean   :1658.1                                    \n 3rd Qu.:2010   3rd Qu.:1990.2                                    \n Max.   :2023   Max.   :2180.4                                    \n natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions\n Min.   : 868.2                                                \n 1st Qu.:1046.6                                                \n Median :1191.4                                                \n Mean   :1218.6                                                \n 3rd Qu.:1302.4                                                \n Max.   :1759.7                                                \n aviation_gasoline_co2_emissions\n Min.   :1.397                  \n 1st Qu.:1.853                  \n Median :2.583                  \n Mean   :2.794                  \n 3rd Qu.:3.294                  \n Max.   :5.771                  \n distillate_fuel_oil_excluding_biodiesel_co2_emissions\n Min.   :418.9                                        \n 1st Qu.:476.3                                        \n Median :543.3                                        \n Mean   :542.8                                        \n 3rd Qu.:604.1                                        \n Max.   :657.6                                        \n hydrocarbon_gas_liquids_co2_emissions jet_fuel_co2_emissions\n Min.   : 71.19                        Min.   :144.1         \n 1st Qu.: 79.41                        1st Qu.:174.7         \n Median : 85.68                        Median :215.2         \n Mean   : 86.80                        Mean   :209.2         \n 3rd Qu.: 95.21                        3rd Qu.:241.6         \n Max.   :111.37                        Max.   :260.6         \n kerosene_co2_emissions lubricants_co2_emissions\n Min.   : 0.784         Min.   : 6.806          \n 1st Qu.: 2.007         1st Qu.:10.742          \n Median : 8.274         Median :12.000          \n Mean   :10.599         Mean   :11.687          \n 3rd Qu.:16.065         3rd Qu.:12.914          \n Max.   :32.747         Max.   :14.717          \n motor_gasoline_excluding_ethanol_co2_emissions petroleum_coke_co2_emissions\n Min.   : 892.5                                 Min.   : 48.33              \n 1st Qu.: 978.6                                 1st Qu.: 56.55              \n Median :1065.0                                 Median : 72.71              \n Mean   :1047.7                                 Mean   : 72.74              \n 3rd Qu.:1127.1                                 3rd Qu.: 81.08              \n Max.   :1216.9                                 Max.   :111.67              \n residual_fuel_oil_co2_emissions other_petroleum_products_co2_emissions\n Min.   : 35.87                  Min.   : 73.79                        \n 1st Qu.: 83.63                  1st Qu.:110.39                        \n Median :149.39                  Median :118.06                        \n Mean   :193.60                  Mean   :117.05                        \n 3rd Qu.:237.29                  3rd Qu.:129.19                        \n Max.   :529.04                  Max.   :150.88                        \n petroleum_excluding_biofuels_co2_emissions total_energy_co2_emissions\n Min.   :1978                               Min.   :4384              \n 1st Qu.:2193                               1st Qu.:4786              \n Median :2255                               Median :5147              \n Mean   :2295                               Mean   :5179              \n 3rd Qu.:2398                               3rd Qu.:5554              \n Max.   :2633                               Max.   :6015              \n\n# Trends in total emissions\nannual_data %&gt;%\n  ggplot(aes(x = year, y = total_energy_co2_emissions)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_smooth(method = \"loess\", color = \"red\", size = 0.8) +\n  labs(\n    title = \"Trend in Total CO2 Emissions (1973-2023)\",\n    x = \"Year\",\n    y = \"Total CO2 Emissions (Million Metric Tons)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFindings:\nThe graph shows emissions steadily rising until 2005, peaking at about 6000 million metric tons. This is followed by a gradual decline, aligning with shifts in energy reliance driven by climate policies and technological innovation. The adoption of renewable energy sources like wind and solar played a crucial role in reversing this trend.\nThe shift around 2005 is particularly significant, as it coincides with major federal and state-level environmental policies aimed at reducing coal use, such as the Clean Air Interstate Rule (CAIR). These efforts, coupled with the economic viability of natural gas due to advancements in hydraulic fracturing, marked a turning point in the U.S. energy sector."
  },
  {
    "objectID": "posts/ghg_stats_final.html#emissions-by-fuel-type",
    "href": "posts/ghg_stats_final.html#emissions-by-fuel-type",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Emissions by Fuel Type",
    "text": "Emissions by Fuel Type\n\n# Bar plot for emissions by fuel type\nannual_data %&gt;%\n  pivot_longer(cols = c(\n    coal_including_coal_coke_net_imports_co2_emissions,\n    natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions,\n    petroleum_excluding_biofuels_co2_emissions\n  ), \n  names_to = \"fuel_type\", \n  values_to = \"emissions\") %&gt;%\n  mutate(fuel_type = case_when(\n    fuel_type == \"coal_including_coal_coke_net_imports_co2_emissions\" ~ \"Coal\",\n    fuel_type == \"natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions\" ~ \"Natural Gas\",\n    fuel_type == \"petroleum_excluding_biofuels_co2_emissions\" ~ \"Petroleum\",\n    TRUE ~ fuel_type\n  )) %&gt;%\n  ggplot(aes(x = fuel_type, y = emissions, fill = as.factor(year))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"CO2 Emissions by Fuel Type (1973-2023)\",\n    x = \"Fuel Type\",\n    y = \"Emissions (Million Metric Tons)\",\n    fill = \"Year\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFindings:\nThe bar plot underscores a sharp decline in coal emissions post-2005, coinciding with increased adoption of natural gas. Despite stable petroleum emissions, the energy landscape has undergone significant transformations, signaling policy-driven shifts and advancements in energy efficiency.\nNatural gas stands out as a ‘bridge fuel.’ Its cleaner than coal, making it a popular choice. But there’s a downside: leaks during production and transport could undo its benefits. This underscores the need for balanced policy approaches that weigh short-term reductions against long-term sustainability.\nAdditionally, the stability of petroleum emissions indicates that the transportation sector remains a significant challenge. The lack of substantial reduction in this area highlights the need for increased adoption of electric vehicles and alternative fuels."
  },
  {
    "objectID": "posts/ghg_stats_final.html#model-setup",
    "href": "posts/ghg_stats_final.html#model-setup",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Model Setup",
    "text": "Model Setup\n\\[\n\\text{Total CO}_2 \\text{ Emissions} = \\beta_0 + \\beta_1(\\text{Year}) + \\beta_2(\\text{Coal Emissions}) + \\beta_3(\\text{Natural Gas Emissions})\n\\]\n\n# Fit a linear model\nlm_model &lt;- lm(\n  total_energy_co2_emissions ~ year + coal_including_coal_coke_net_imports_co2_emissions + natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions,\n  data = annual_data\n)\n\n# Model summary\nsummary(lm_model)\n\n\nCall:\nlm(formula = total_energy_co2_emissions ~ year + coal_including_coal_coke_net_imports_co2_emissions + \n    natural_gas_excluding_supplemental_gaseous_fuels_co2_emissions, \n    data = annual_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-183.90 -106.54  -11.34   65.39  324.11 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     2.404e+04\nyear                                                           -1.191e+01\ncoal_including_coal_coke_net_imports_co2_emissions              1.430e+00\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions  2.110e+00\n                                                               Std. Error\n(Intercept)                                                     6.593e+03\nyear                                                            3.484e+00\ncoal_including_coal_coke_net_imports_co2_emissions              7.308e-02\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions  2.480e-01\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      3.647 0.000664\nyear                                                            -3.420 0.001306\ncoal_including_coal_coke_net_imports_co2_emissions              19.570  &lt; 2e-16\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions   8.510 4.47e-11\n                                                                  \n(Intercept)                                                    ***\nyear                                                           ** \ncoal_including_coal_coke_net_imports_co2_emissions             ***\nnatural_gas_excluding_supplemental_gaseous_fuels_co2_emissions ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 126.9 on 47 degrees of freedom\nMultiple R-squared:  0.9325,    Adjusted R-squared:  0.9282 \nF-statistic: 216.5 on 3 and 47 DF,  p-value: &lt; 2.2e-16\n\n\nFindings:\nRegression analysis reveals:\n\nCoal and Natural Gas as Key Predictors: Coal (β = 1.43) and natural gas (β = 2.11) emissions are significant predictors of total emissions. These coefficients underscore their critical role in shaping national emissions profiles.\nTemporal Decline: The year variable (β = -11.91) reflects a downward trend, aligning with broader shifts toward decarbonization.\nModel Strength: An adjusted R² of 0.93 indicates the model effectively explains 93% of total emissions variation.\n\nThe significant coefficients validate the importance of coal and natural gas in determining overall GHG emissions. The results highlight that while natural gas is contributing to reductions in total emissions, its overall impact is moderated by its role in replacing coal rather than being phased out itself."
  },
  {
    "objectID": "posts/ghg_stats_final.html#model-diagnostics",
    "href": "posts/ghg_stats_final.html#model-diagnostics",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\n# Durbin-Watson Test\nlibrary(lmtest)\ndwtest(lm_model)\n\n\n    Durbin-Watson test\n\ndata:  lm_model\nDW = 0.46072, p-value = 9.964e-14\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Residual plots\nplot(lm_model, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFindings:\nThe residual diagnostics point to positive autocorrelation (DW = 0.46, p &lt; 0.001). In simpler terms, the data suggests that we need more sophisticated models, like ARIMA, to make better predictions.\nPositive autocorrelation in the residuals suggests that omitted variables or structural shifts in energy markets may influence the model. Addressing these factors would enhance the predictive capacity of future analyses."
  },
  {
    "objectID": "posts/ghg_stats_final.html#arima-modeling",
    "href": "posts/ghg_stats_final.html#arima-modeling",
    "title": "Greenhouse Gas Emissions Analysis",
    "section": "ARIMA Modeling",
    "text": "ARIMA Modeling\n\n# Fit ARIMA model\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nemissions_ts &lt;- ts(annual_data$total_energy_co2_emissions, start = min(annual_data$year), frequency = 1)\narima_model &lt;- auto.arima(emissions_ts)\n\n# Forecast\nforecast(arima_model, h = 10) %&gt;%\n  autoplot() +\n  labs(\n    title = \"ARIMA Forecast for Total CO2 Emissions\",\n    x = \"Year\",\n    y = \"Total CO2 Emissions (Million Metric Tons)\"\n  )\n\n\n\n\n\n\n\n\nFindings:\nThe ARIMA forecast suggests that the emissions will continue to drop at a slow rate, but with increasing variability of the predictions over time. This is consistent with the importance of the efforts for the decarbonization, which emphasise the importance of policies in the process.\nThis shows the limitations of depending on past tendencies, for example, external events such as technological advancements, changes in the economy or policies across the globe may affect these predictions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Takeen Shamloo",
    "section": "",
    "text": "Hello! My name is Takeen Shamloo, I am an aspiring Data Scientist and I love to program all sorts of things. I specialize in mobile apps and I don’t shy away from a good challenge."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Visualizing and Understanding the Thomas Fire",
    "section": "",
    "text": "By: Takeen Shamloo\nDate: Dec 4th, 2024\nClass: EDS 220\nThomas Fire GitHub Repo\n\n\nThe Thomas Fire, one of California’s largest wildfires significantly impacted air quality in Santa Barbara County during December 2017. We are going to dive into two different ways of looking at the fire’s effects:\n\nAir Quality Analysis: We’ll look at AQI trends during 2017-2018 to see how the fire impacted air quality.\nFalse Color Imagery: We’ll use satellite data to get a closer look at how the fire changed the landscape, like burn scars and vegetation health.\n\nFalse Color Imagery is like using a filter on satellite photos to highlight things you can’t see with just your eyes. This way we can spot areas that were burned or stressed during and after the fire. Combining this with AQI analysis gives us a better idea of how the fire impacted both the air and the land.\n Source: NBC News\n\n\nThe following code uses data from the sources below:\n\nAir Quality Index (AQI) Dataset\nThis dataset contains daily AQI measurements for counties across the United States, used to analyze how the Thomas Fire affected air quality in Santa Barbara County.\nU.S. Environmental Protection Agency. (n.d.). Air Quality Index (AQI) Data [Data set]. Retrieved December 4, 2024, from https://aqs.epa.gov/aqsweb/airdata/download_files.html#Daily\nLandsat Bands Dataset\nThis dataset includes a simplified collection of bands (Red, Green, Blue, Near-Infrared, and Shortwave Infrared) from Landsat 8, used to create True Color and False Color images for the Thomas Fire area.\nMicrosoft. (n.d.). Landsat Collection 2 Level-2 (C2 L2) Data [Data set]. Microsoft Planetary Computer. Retrieved December 4, 2024, from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2\nFire Perimeter Dataset\nThis dataset provides geospatial boundaries for fire perimeters in California. For this analysis, the data was filtered to focus on the 2017 Thomas Fire.\nU.S. Government. (n.d.). California Fire Perimeters (ALL) [Data set]. Data.gov. Retrieved December 4, 2024, from https://catalog.data.gov/dataset/california-fire-perimeters-all\n\n\n\n\n\nThe workflow begins as follows:\n\n\n\n# Importing necessary libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Additional Libraries for Part 2 \nimport rioxarray as rioxr\nimport os\nimport geopandas as gpd\nfrom rasterio.plot import show\nimport fiona\nfrom shapely.geometry import shape\n\n# Remove warnings\nwarnings.filterwarnings('ignore')\n\nWe’re using three Python libraries to make all this happen: - pandas: For loading, cleaning, and organizing our data. - numpy: Used here for some extra math functions. - matplotlib.pyplot: The go-to library for creating plots. This is how we’ll bring the AQI trends to life with visuals.\nAdditional Libraries (For Part 2): - rioxarray: This one’s a powerhouse for working with geospatial raster data, like satellite images. We use it for loading and visualizing Landsat data which is typically stored in raster formats like NetCDF. - os: Helps us handle file paths and manage data directories. - geopandas: Used to process and visualize the geospatial fire perimeter data. - fiona: Handles shapefile formats to load the Thomas Fire boundary data which we overlay on our maps. We used since we had a version conflict with geopandas built in version of this same package. - shapely: Analyzes geometric objects, like polygons, to map the fire perimeter.\n\n\n\n\nDescription:\nFor this section we will be analyzing air quality info (AQI) during the Thomas Fire. By looking at daily AQI and calculating a 5-day rolling average we can see how the fire impacted air quality in Santa Barbara County.\nThe AQI dataset includes daily air quality measurements for every county in the U.S. These values are essential for spotting anomalies and understanding how wildfire events affect local air quality.\n\n\n\n# Loading the data\n#aqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\", compression='zip')\n#aqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\", compression='zip')\n\n# Server was down had to used downloaded csv files of the data\naqi_17 = pd.read_csv(\"data/daily_aqi_by_county_2017.csv\")\naqi_18 = pd.read_csv(\"data/daily_aqi_by_county_2018.csv\")\n\nSo, first off, we couldn’t grab the data from the internet directly because the server was down but I have left the commented out code if needed. Instead, downloaded the data locally as CSV files. They have all the air quality info (AQI) for every county in the U.S. for 2017 and 2018. Each file is a year’s worth of daily AQI data.\n\n\n\n\n# Combining datasets and cleaning column names\naqi = pd.concat([aqi_17, aqi_18])\naqi.columns = aqi.columns.str.lower().str.replace(' ', '_')\n\nHere we’re smushing the 2017 and 2018 data into one big dataset so we can look at everything together. I cleaned up the column names too and made them lowercase as well as swapped out spaces for underscores. This step is imperative as it will make the data a lot easier to work with later.\n\n\n\n\n# Filtering for Santa Barbara County\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\naqi_sb = aqi_sb.drop(['state_name', 'county_name', 'state_code', 'county_code'], axis=1)\n\nThis step is fairly simple we only care about Santa Barbara County, so I filtered out the rest of the data. Also, I ditched some columns like state name and county code since we don’t really need those. No point keeping extra stuff around that could potentially confuse us later.\n\n\n\n\n# Converting date column to datetime and setting it as the index\naqi_sb['date'] = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\n\nUsing the code above I made sure the date column is in a proper date format because we’re working with time-series data (fancy word for data over time). Then I set it as the index which just means we can use the dates to organize the data.\n\n\n\n\n# Calculating a 5-day rolling average for AQI\naqi_sb['five_day_average'] = aqi_sb['aqi'].rolling('5D').mean()\n\nThis is where the fun begins! I calculated a 5-day rolling average. Basically, it smooths out the daily ups and downs so we can see bigger trends. It’s like looking at the bigger picture instead of freaking out about every little bump.\n\n\n\n\n# Plotting the results\nplt.figure(figsize=(8, 4))\nplt.plot(aqi_sb.index.values, aqi_sb['aqi'].values, label='Daily AQI', color='green', linewidth=1.5)\nplt.plot(aqi_sb.index.values, aqi_sb['five_day_average'].values, label='5-Day Average AQI', color='red', linestyle='--', linewidth=1.5)\nplt.title('Daily and 5 Day Rolling Average AQI in Santa Barbara')\nplt.xlabel('Date (2017-18)')\nplt.ylabel('AQI')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nBased on my visualization, the graph to show what’s happening with AQI. The green line is the daily AQI which jumps around a lot. The red dashed line is the 5-day rolling average which is way smoother. You can see a massive spike in December 2017 because of the Thomas Fire we can see AQI at a peak reading of ~265 AQI score which is way over the ~50 AQI score we can see assume off the baseline."
  },
  {
    "objectID": "posts/index.html#about-the-analysis",
    "href": "posts/index.html#about-the-analysis",
    "title": "Visualizing and Understanding the Thomas Fire",
    "section": "",
    "text": "The Thomas Fire, one of California’s largest wildfires significantly impacted air quality in Santa Barbara County during December 2017. We are going to dive into two different ways of looking at the fire’s effects:\n\nAir Quality Analysis: We’ll look at AQI trends during 2017-2018 to see how the fire impacted air quality.\nFalse Color Imagery: We’ll use satellite data to get a closer look at how the fire changed the landscape, like burn scars and vegetation health.\n\nFalse Color Imagery is like using a filter on satellite photos to highlight things you can’t see with just your eyes. This way we can spot areas that were burned or stressed during and after the fire. Combining this with AQI analysis gives us a better idea of how the fire impacted both the air and the land.\n Source: NBC News\n\n\nThe following code uses data from the sources below:\n\nAir Quality Index (AQI) Dataset\nThis dataset contains daily AQI measurements for counties across the United States, used to analyze how the Thomas Fire affected air quality in Santa Barbara County.\nU.S. Environmental Protection Agency. (n.d.). Air Quality Index (AQI) Data [Data set]. Retrieved December 4, 2024, from https://aqs.epa.gov/aqsweb/airdata/download_files.html#Daily\nLandsat Bands Dataset\nThis dataset includes a simplified collection of bands (Red, Green, Blue, Near-Infrared, and Shortwave Infrared) from Landsat 8, used to create True Color and False Color images for the Thomas Fire area.\nMicrosoft. (n.d.). Landsat Collection 2 Level-2 (C2 L2) Data [Data set]. Microsoft Planetary Computer. Retrieved December 4, 2024, from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2\nFire Perimeter Dataset\nThis dataset provides geospatial boundaries for fire perimeters in California. For this analysis, the data was filtered to focus on the 2017 Thomas Fire.\nU.S. Government. (n.d.). California Fire Perimeters (ALL) [Data set]. Data.gov. Retrieved December 4, 2024, from https://catalog.data.gov/dataset/california-fire-perimeters-all"
  },
  {
    "objectID": "posts/index.html#organized-analysis",
    "href": "posts/index.html#organized-analysis",
    "title": "Visualizing and Understanding the Thomas Fire",
    "section": "",
    "text": "The workflow begins as follows:\n\n\n\n# Importing necessary libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Additional Libraries for Part 2 \nimport rioxarray as rioxr\nimport os\nimport geopandas as gpd\nfrom rasterio.plot import show\nimport fiona\nfrom shapely.geometry import shape\n\n# Remove warnings\nwarnings.filterwarnings('ignore')\n\nWe’re using three Python libraries to make all this happen: - pandas: For loading, cleaning, and organizing our data. - numpy: Used here for some extra math functions. - matplotlib.pyplot: The go-to library for creating plots. This is how we’ll bring the AQI trends to life with visuals.\nAdditional Libraries (For Part 2): - rioxarray: This one’s a powerhouse for working with geospatial raster data, like satellite images. We use it for loading and visualizing Landsat data which is typically stored in raster formats like NetCDF. - os: Helps us handle file paths and manage data directories. - geopandas: Used to process and visualize the geospatial fire perimeter data. - fiona: Handles shapefile formats to load the Thomas Fire boundary data which we overlay on our maps. We used since we had a version conflict with geopandas built in version of this same package. - shapely: Analyzes geometric objects, like polygons, to map the fire perimeter."
  },
  {
    "objectID": "posts/index.html#part-1-air-quality-analysis-during-the-thomas-fire",
    "href": "posts/index.html#part-1-air-quality-analysis-during-the-thomas-fire",
    "title": "Visualizing and Understanding the Thomas Fire",
    "section": "",
    "text": "Description:\nFor this section we will be analyzing air quality info (AQI) during the Thomas Fire. By looking at daily AQI and calculating a 5-day rolling average we can see how the fire impacted air quality in Santa Barbara County.\nThe AQI dataset includes daily air quality measurements for every county in the U.S. These values are essential for spotting anomalies and understanding how wildfire events affect local air quality.\n\n\n\n# Loading the data\n#aqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\", compression='zip')\n#aqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\", compression='zip')\n\n# Server was down had to used downloaded csv files of the data\naqi_17 = pd.read_csv(\"data/daily_aqi_by_county_2017.csv\")\naqi_18 = pd.read_csv(\"data/daily_aqi_by_county_2018.csv\")\n\nSo, first off, we couldn’t grab the data from the internet directly because the server was down but I have left the commented out code if needed. Instead, downloaded the data locally as CSV files. They have all the air quality info (AQI) for every county in the U.S. for 2017 and 2018. Each file is a year’s worth of daily AQI data.\n\n\n\n\n# Combining datasets and cleaning column names\naqi = pd.concat([aqi_17, aqi_18])\naqi.columns = aqi.columns.str.lower().str.replace(' ', '_')\n\nHere we’re smushing the 2017 and 2018 data into one big dataset so we can look at everything together. I cleaned up the column names too and made them lowercase as well as swapped out spaces for underscores. This step is imperative as it will make the data a lot easier to work with later.\n\n\n\n\n# Filtering for Santa Barbara County\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\naqi_sb = aqi_sb.drop(['state_name', 'county_name', 'state_code', 'county_code'], axis=1)\n\nThis step is fairly simple we only care about Santa Barbara County, so I filtered out the rest of the data. Also, I ditched some columns like state name and county code since we don’t really need those. No point keeping extra stuff around that could potentially confuse us later.\n\n\n\n\n# Converting date column to datetime and setting it as the index\naqi_sb['date'] = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\n\nUsing the code above I made sure the date column is in a proper date format because we’re working with time-series data (fancy word for data over time). Then I set it as the index which just means we can use the dates to organize the data.\n\n\n\n\n# Calculating a 5-day rolling average for AQI\naqi_sb['five_day_average'] = aqi_sb['aqi'].rolling('5D').mean()\n\nThis is where the fun begins! I calculated a 5-day rolling average. Basically, it smooths out the daily ups and downs so we can see bigger trends. It’s like looking at the bigger picture instead of freaking out about every little bump.\n\n\n\n\n# Plotting the results\nplt.figure(figsize=(8, 4))\nplt.plot(aqi_sb.index.values, aqi_sb['aqi'].values, label='Daily AQI', color='green', linewidth=1.5)\nplt.plot(aqi_sb.index.values, aqi_sb['five_day_average'].values, label='5-Day Average AQI', color='red', linestyle='--', linewidth=1.5)\nplt.title('Daily and 5 Day Rolling Average AQI in Santa Barbara')\nplt.xlabel('Date (2017-18)')\nplt.ylabel('AQI')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nBased on my visualization, the graph to show what’s happening with AQI. The green line is the daily AQI which jumps around a lot. The red dashed line is the 5-day rolling average which is way smoother. You can see a massive spike in December 2017 because of the Thomas Fire we can see AQI at a peak reading of ~265 AQI score which is way over the ~50 AQI score we can see assume off the baseline."
  },
  {
    "objectID": "posts/HW4-blog.html",
    "href": "posts/HW4-blog.html",
    "title": "EDS 240: Homework #4",
    "section": "",
    "text": "California is no stranger to dry weather and strict water regulations. Yet despite these restrictive measures, getting a clear picture of whats happening often means sifting through dense academic papers or navigating clunky government portals. Adding to this challenge is the know how required to work with the data.\nMotivated by this need for accessible view of California’s water and where it comes from, my project aims to take a look at a few water usage trends across the state. Using publicly available data from the California Water Data Consortium my goal is to help answer the following questions:\n1. How have statewide water shortages evolved over recent years, and how severe have they been? (Target Audience: California State Policy Makers) from: actual_water_shortage_level\n\nI am using actual_water_shortage_level.csv which tracks water shortages in California. The dataset includes reports from public water systems with assigned shortage levels.\nI used start_date to determine the year and grouped the data by state_standard_shortage_level which organizes shortages from 0 (no shortage) to 6 (severe shortage). I calculated the total number of reported shortages per level each year and converted them into percentages for easier comparison. The donut chart shows how shortages are distributed over time, with the yearly average shortage level displayed in the center.\nVariables used:\n\nstart_date – Extracted to track shortages by year.\n\nstate_standard_shortage_level – Represents the shortage level category.\n\nCalculated Variables:\n\ntotal_shortage – Counts occurrences of each shortage level.\n\npercentage – Normalized shortage levels per year.\n\nmean_shortage – Yearly average shortage level.\n\nHere we can see a visualization of the 2022-2024 and a spread of call the counties by their reported shortage level indicator for that year. In the middle of our graph we can see the overall average shortage indicator for that year. From this we can see that the shortage value on average is steadily decreasing over time dropping from 1.73 in 2022 to 1.04 in 2024. Thats overall good news as it means that on average most of the counties in california fall around ~10% shortage. Unfortunately our dataset was limited to only years 2022-24 hence the selected range.\n2. Which water system produces the most water each year and how do they change over time? Are they still the highest producers? (Target Audience: Environmental Researchers) from: historical_production_delivery\n\nI used historical_production_delivery.csv which records water production and delivery for public water systems.\nTo find the top producers I filtered for water_produced_or_delivered == \"water produced\" and grouped the data by water_system_name and start_date. I then summed quantity_acre_feet per year to determine the top producer annually. I pulled the full production history of each top producer and plotted their trends over time.\nThe area chart compares these top producers layering them so the largest producer appears in the background while smaller producers overlay on top.\nVariables used:\n\nstart_date – Extracted to track production by year.\n\nwater_system_name – Identifies the producing water system.\n\nwater_produced_or_delivered – Filters for \"water produced\".\n\nquantity_acre_feet – Total amount of water produced.\n\nCalculated Variables:\n\ntotal_produced – Sum of quantity_acre_feet per system each year.\n\ntotal_produced_m – Converted to millions for easier visualization.\n\nFor this plot I wanted to see the number 1 producer (total annual) of each year and how they compare with each other or if they are still the number one producer across other years. I also added points for the top produced month of each supplier for each year as reference as some were too low to be noticed while others were easily distinguishable. Our data is compacted to a factor of millions so the really low productions years are not as noticeable as seen in 2014 and a few years after but thats not the point of this graph. We want to see the peaks and whos producing them. Interesting thing to note is that not one producer kept the crown of top producer for more than one year. Also to note there were a plethora of other suppliers but for the sake of narrowing the scope to get a general picture of our producers it was necessary to focus on top producers.\n3. What types of water facilities exist in my City (in California)? (Target Audience: Local Government & Water Management Agencies) from: source_name\n\nI used source_name.csv and California_Drinking_Water_System_Area_Boundaries.shp to map water facility types across California.\nThe dataset includes source_facility_type which classifies each facility as a well, spring, reservoir, or other type. The dataset also provides latitude and longitude, which I used to plot facilities on a static map.\nTo focus on key facility types I filtered for \"well\", \"spring\", and \"reservoir\". I overlaid these on CA_polygon which provides city boundaries. This map shows the locations of different facility types within cities.\nVariables used:\n\nsource_facility_type – Categorizes facilities.\n\nlatitude, longitude – Used to plot facility locations.\n\nBoundary Variables:\n\nCA_polygon – Provides city boundary overlays.\n\nFor this chart I simply wanted to share where these water sources are and what they are. This is a simplified version focusing on our key facility types, but it still shows you the sheer volume of wells and how a large chunk of California relies on such a key piece of infrastructure. Other facility types were not relevant to our data process as they were either too niche or commercial features that would not be able to be grouped with our sources. We can see a few reservoirs and almost no springs which makes sense given the efficacy of wells and the fact that there are geographically a much smaller in comparison (generally speaking).\nBelow is the full code for your to go through an recreate the visualizations we have discussed in this analysis.\n\n# | code-fold: true\n# | eval: false\n# | echo: true\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/takeenshamloo/MEDS 2024-25/MyWebsite/takeenshamloo.github.io\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(viridis)  \n\nLoading required package: viridisLite\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(spnaf)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n# Load actual shortage data.\nactual_shortage_data &lt;- read_csv(here(\"data\", \"actual_water_shortage_level.csv\"))\n\nRows: 12313 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): pwsid, supplier_name\ndbl  (2): org_id, state_standard_shortage_level\ndate (2): start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Convert start_date to year format.\nactual_shortage_data &lt;- actual_shortage_data %&gt;%\n  mutate(year = year(as.Date(start_date)))\n\n# Remove NA shortage levels.\nactual_shortage_data &lt;- actual_shortage_data %&gt;%\n  filter(!is.na(state_standard_shortage_level))\n\n# Summarize data to get total shortage per level per year.\nshortage_summary &lt;- actual_shortage_data %&gt;%\n  group_by(year, state_standard_shortage_level) %&gt;%\n  summarise(total_shortage = n(), .groups = \"drop\")\n\n# Scale shortages to percentages so each year sums to 100%.\nshortage_summary &lt;- shortage_summary %&gt;%\n  group_by(year) %&gt;%\n  mutate(percentage = (total_shortage / sum(total_shortage)) * 100)  # Convert to % per year\n\n# Compute average shortage level per year.\naverage_shortage &lt;- actual_shortage_data %&gt;%\n  group_by(year) %&gt;%\n  summarise(mean_shortage = round(mean(state_standard_shortage_level, na.rm = TRUE), 2))\n\n# Add details for legend for readability.\nlevels &lt;- c(\n  \"0\" = \"0 (No Shortage Level Invoked)\",\n  \"1\" = \"1 (Less than 10% Shortage)\", \n  \"2\" = \"2 (10-19% Shortage)\", \n  \"3\" = \"3 (20-29% Shortage)\", \n  \"4\" = \"4 (30-39% Shortage)\", \n  \"5\" = \"5 (40-49% Shortage)\", \n  \"6\" = \"6 (Greater than 50% Shortage)\"\n)\n\n# to flip colors in scale_fill(direction = -1)\n# Create the donut chart.\nggplot(shortage_summary, aes(x = \"\", y = percentage, fill = factor(state_standard_shortage_level))) +\n  geom_bar(stat = \"identity\", width = 1) +  # White border for separation.\n  coord_polar(theta = \"y\", start = 0) +  # Convert to donut chart.\n  facet_wrap(~ year, ncol = 3) +  # Separate donuts for each year.\n  scale_fill_viridis(discrete = TRUE, option = \"magma\", labels = levels) +  # Use viridis magma scale\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 12, face = \"bold\"),\n    legend.text = element_text(size = 10),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5)\n  ) +\n  labs(\n    title = \"Proportioned Water Shortage Levels by Year\",\n    subtitle = \"Center Represents Yearly Mean Shortage Level\",\n    fill = \"Shortage Level\"\n  ) +\n  \n  # Add the white circle in the middle to create the donut hole.\n  annotate(\"point\", x = 0, y = 0, size = 30, color = \"white\") +\n\n  # Add mean shortage level text in the center of each donut.\n  geom_text(data = average_shortage, aes(x = 0, y = 0, label = mean_shortage), \n            color = \"black\", size = 10, fontface = \"bold\", inherit.aes = FALSE)\n\n\n\n\n\n\n\n# Load historical data.\nhistorical_df &lt;- read_csv(here(\"data\", \"historical_production_delivery.csv\"))\n\nRows: 902112 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): pwsid, water_system_name, water_produced_or_delivered, water_type\ndbl  (2): org_id, quantity_acre_feet\ndate (2): start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Get year from start_date.\nhistorical_df &lt;- historical_df %&gt;%\n  mutate(year = year(as.Date(start_date)))\n\n# Identify the #1 producer for each year.\ntop_producer_per_year &lt;- historical_df %&gt;%\n  filter(water_produced_or_delivered == \"water produced\") %&gt;%  # Filter only production.\n  group_by(year, water_system_name) %&gt;%\n  summarise(total_produced = sum(quantity_acre_feet, na.rm = TRUE), .groups = \"drop\") %&gt;%  # Sum per year.\n  arrange(year, desc(total_produced)) %&gt;%  # Sort within each year.\n  group_by(year) %&gt;%\n  slice_head(n = 1)  # Select the #1 producer for each year.\n\n# Get full time series for these top producers.\ntop_producer_trends &lt;- historical_df %&gt;%\n  filter(water_produced_or_delivered == \"water produced\") %&gt;%  # Only produced water\n  semi_join(top_producer_per_year, by = \"water_system_name\") %&gt;%  # Keep only top producers\n  group_by(year, start_date, water_system_name) %&gt;%\n  summarise(total_produced = sum(quantity_acre_feet, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(total_produced_m = total_produced / 1e6)  # Convert to millions\n\n# Sort water systems by total production so the largest is plotted last. \n# Might change later...\ntotal_produced_order &lt;- top_producer_trends %&gt;%\n  group_by(water_system_name) %&gt;%\n  summarise(total_produced = sum(total_produced_m, na.rm = TRUE)) %&gt;%\n  arrange(total_produced) %&gt;%  # Smallest producer first, largest last. (might change)\n  pull(water_system_name)\n\n# To figure out which were the top months by for our suppliers that year.\n# Originally wanted to do top monthly producer plotted with a top yearly\n# producer icon, but was having trouble imagining the calculations and how to\n# get it to plot onto geom_area with our top_producer_trends.\n# to fix if resubmitting. \ntop_producer_month_points &lt;- top_producer_trends %&gt;%\n  left_join(\n    top_producer_per_year %&gt;% \n      select(year, water_system_name),\n    by = c(\"year\", \"water_system_name\")\n  ) %&gt;%\n  filter(!is.na(total_produced))  %&gt;%\n  group_by(year, water_system_name) %&gt;%\n  filter(total_produced == max(total_produced)) %&gt;%\n  slice(1) %&gt;%\n  ungroup()\n\n# Convert to factor with proper order.\ntop_producer_trends$water_system_name &lt;- factor(top_producer_trends$water_system_name, levels = total_produced_order)\n\n# Plot area chart with layered fills.\nggplot(top_producer_trends, aes(x = as.Date(start_date), y = total_produced_m, fill = water_system_name)) +\n  geom_area() +  # Transparent fills, no stacking\n  # Add a special symbol for each top producer's peak month\n  geom_point(\n    data = top_producer_month_points,\n    aes(x = as.Date(start_date), y = total_produced_m),\n    position = position_stack(),\n    shape = 21,       # or 22, 23, etc. 21 is a nice circle with fill\n    size = 3,\n    color = \"black\",  # outline color\n  ) +\n  labs(\n    title = \"Water Production Trends of Yearly #1 Producers\",\n    subtitle = \"Lowest overall producers appear on top, largest producers in the background\",\n    x = \"Time\",\n    y = \"Water Produced (Million Acre-Feet)\",\n    fill = \"Top Producer\",\n    color = \"Top Producer\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nsource_name &lt;- read_csv(here(\"data\", \"source_name.csv\")) |&gt;\n    clean_names()\n\nRows: 10650 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): pwsid, source_facility_id, source_facility_name, source_facility_ac...\ndbl (3): org_id, latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsource_geo &lt;- source_name %&gt;% \n    filter(!is.na(latitude)) %&gt;% \n    filter(!is.na(org_id))\n\n# Load and clean source facility data\nsource_geo &lt;- source_geo %&gt;%\n  select(-c(\"source_facility_name\", \"source_facility_activity_status\", \"source_facility_availability\", \"source_facility_id\"))\n\n# Convert source facility coordinates to spatial format\nsource_geo &lt;- st_as_sf(source_geo, coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4269\") \n\n# Validate CRS of CA_polygon\nst_crs(CA_polygon)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n# Select facility types of interest.\nselected_facility_types &lt;- c(\"well\", \"spring\", \"reservoir\")  # Limit filter for now.\nfiltered_source_geo &lt;- source_geo %&gt;%\n  filter(source_facility_type %in% selected_facility_types)\n\ntmap_mode(\"plot\")  \n\ntmap mode set to plotting\n\n# Build base map.\nbase_map &lt;- \n  tm_shape(CA_polygon) +\n  tm_borders()\n\n# If no data, plot only the base map.\nif (nrow(filtered_source_geo) == 0) {\n  print(base_map)\n} else {\n  base_map +\n    tm_shape(filtered_source_geo) +\n    tm_symbols(\n      col = \"source_facility_type\",  # Color points by facility type.\n      palette = \"Set1\", \n      size = 0.1,\n      title.col = \"Facility Type\"\n    ) +\n    tm_layout(\n      title = \"Water Facilities in California\",\n      title.position = c(\"left\", \"top\"),\n      legend.outside = TRUE,  # Move legend outside the plot.\n      legend.outside.position = \"right\",  # Place legend on the right side.\n      legend.frame = FALSE  # Remove legend border for cleaner look.\n    )\n}"
  },
  {
    "objectID": "posts/HW4-blog.html#introduction",
    "href": "posts/HW4-blog.html#introduction",
    "title": "EDS 240: Homework #4",
    "section": "",
    "text": "California is no stranger to dry weather and strict water regulations. Yet despite these restrictive measures, getting a clear picture of whats happening often means sifting through dense academic papers or navigating clunky government portals. Adding to this challenge is the know how required to work with the data.\nMotivated by this need for accessible view of California’s water and where it comes from, my project aims to take a look at a few water usage trends across the state. Using publicly available data from the California Water Data Consortium my goal is to help answer the following questions:\n1. How have statewide water shortages evolved over recent years, and how severe have they been? (Target Audience: California State Policy Makers) from: actual_water_shortage_level\n\nI am using actual_water_shortage_level.csv which tracks water shortages in California. The dataset includes reports from public water systems with assigned shortage levels.\nI used start_date to determine the year and grouped the data by state_standard_shortage_level which organizes shortages from 0 (no shortage) to 6 (severe shortage). I calculated the total number of reported shortages per level each year and converted them into percentages for easier comparison. The donut chart shows how shortages are distributed over time, with the yearly average shortage level displayed in the center.\nVariables used:\n\nstart_date – Extracted to track shortages by year.\n\nstate_standard_shortage_level – Represents the shortage level category.\n\nCalculated Variables:\n\ntotal_shortage – Counts occurrences of each shortage level.\n\npercentage – Normalized shortage levels per year.\n\nmean_shortage – Yearly average shortage level.\n\nHere we can see a visualization of the 2022-2024 and a spread of call the counties by their reported shortage level indicator for that year. In the middle of our graph we can see the overall average shortage indicator for that year. From this we can see that the shortage value on average is steadily decreasing over time dropping from 1.73 in 2022 to 1.04 in 2024. Thats overall good news as it means that on average most of the counties in california fall around ~10% shortage. Unfortunately our dataset was limited to only years 2022-24 hence the selected range.\n2. Which water system produces the most water each year and how do they change over time? Are they still the highest producers? (Target Audience: Environmental Researchers) from: historical_production_delivery\n\nI used historical_production_delivery.csv which records water production and delivery for public water systems.\nTo find the top producers I filtered for water_produced_or_delivered == \"water produced\" and grouped the data by water_system_name and start_date. I then summed quantity_acre_feet per year to determine the top producer annually. I pulled the full production history of each top producer and plotted their trends over time.\nThe area chart compares these top producers layering them so the largest producer appears in the background while smaller producers overlay on top.\nVariables used:\n\nstart_date – Extracted to track production by year.\n\nwater_system_name – Identifies the producing water system.\n\nwater_produced_or_delivered – Filters for \"water produced\".\n\nquantity_acre_feet – Total amount of water produced.\n\nCalculated Variables:\n\ntotal_produced – Sum of quantity_acre_feet per system each year.\n\ntotal_produced_m – Converted to millions for easier visualization.\n\nFor this plot I wanted to see the number 1 producer (total annual) of each year and how they compare with each other or if they are still the number one producer across other years. I also added points for the top produced month of each supplier for each year as reference as some were too low to be noticed while others were easily distinguishable. Our data is compacted to a factor of millions so the really low productions years are not as noticeable as seen in 2014 and a few years after but thats not the point of this graph. We want to see the peaks and whos producing them. Interesting thing to note is that not one producer kept the crown of top producer for more than one year. Also to note there were a plethora of other suppliers but for the sake of narrowing the scope to get a general picture of our producers it was necessary to focus on top producers.\n3. What types of water facilities exist in my City (in California)? (Target Audience: Local Government & Water Management Agencies) from: source_name\n\nI used source_name.csv and California_Drinking_Water_System_Area_Boundaries.shp to map water facility types across California.\nThe dataset includes source_facility_type which classifies each facility as a well, spring, reservoir, or other type. The dataset also provides latitude and longitude, which I used to plot facilities on a static map.\nTo focus on key facility types I filtered for \"well\", \"spring\", and \"reservoir\". I overlaid these on CA_polygon which provides city boundaries. This map shows the locations of different facility types within cities.\nVariables used:\n\nsource_facility_type – Categorizes facilities.\n\nlatitude, longitude – Used to plot facility locations.\n\nBoundary Variables:\n\nCA_polygon – Provides city boundary overlays.\n\nFor this chart I simply wanted to share where these water sources are and what they are. This is a simplified version focusing on our key facility types, but it still shows you the sheer volume of wells and how a large chunk of California relies on such a key piece of infrastructure. Other facility types were not relevant to our data process as they were either too niche or commercial features that would not be able to be grouped with our sources. We can see a few reservoirs and almost no springs which makes sense given the efficacy of wells and the fact that there are geographically a much smaller in comparison (generally speaking).\nBelow is the full code for your to go through an recreate the visualizations we have discussed in this analysis.\n\n# | code-fold: true\n# | eval: false\n# | echo: true\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/takeenshamloo/MEDS 2024-25/MyWebsite/takeenshamloo.github.io\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(viridis)  \n\nLoading required package: viridisLite\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(spnaf)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n# Load actual shortage data.\nactual_shortage_data &lt;- read_csv(here(\"data\", \"actual_water_shortage_level.csv\"))\n\nRows: 12313 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): pwsid, supplier_name\ndbl  (2): org_id, state_standard_shortage_level\ndate (2): start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Convert start_date to year format.\nactual_shortage_data &lt;- actual_shortage_data %&gt;%\n  mutate(year = year(as.Date(start_date)))\n\n# Remove NA shortage levels.\nactual_shortage_data &lt;- actual_shortage_data %&gt;%\n  filter(!is.na(state_standard_shortage_level))\n\n# Summarize data to get total shortage per level per year.\nshortage_summary &lt;- actual_shortage_data %&gt;%\n  group_by(year, state_standard_shortage_level) %&gt;%\n  summarise(total_shortage = n(), .groups = \"drop\")\n\n# Scale shortages to percentages so each year sums to 100%.\nshortage_summary &lt;- shortage_summary %&gt;%\n  group_by(year) %&gt;%\n  mutate(percentage = (total_shortage / sum(total_shortage)) * 100)  # Convert to % per year\n\n# Compute average shortage level per year.\naverage_shortage &lt;- actual_shortage_data %&gt;%\n  group_by(year) %&gt;%\n  summarise(mean_shortage = round(mean(state_standard_shortage_level, na.rm = TRUE), 2))\n\n# Add details for legend for readability.\nlevels &lt;- c(\n  \"0\" = \"0 (No Shortage Level Invoked)\",\n  \"1\" = \"1 (Less than 10% Shortage)\", \n  \"2\" = \"2 (10-19% Shortage)\", \n  \"3\" = \"3 (20-29% Shortage)\", \n  \"4\" = \"4 (30-39% Shortage)\", \n  \"5\" = \"5 (40-49% Shortage)\", \n  \"6\" = \"6 (Greater than 50% Shortage)\"\n)\n\n# to flip colors in scale_fill(direction = -1)\n# Create the donut chart.\nggplot(shortage_summary, aes(x = \"\", y = percentage, fill = factor(state_standard_shortage_level))) +\n  geom_bar(stat = \"identity\", width = 1) +  # White border for separation.\n  coord_polar(theta = \"y\", start = 0) +  # Convert to donut chart.\n  facet_wrap(~ year, ncol = 3) +  # Separate donuts for each year.\n  scale_fill_viridis(discrete = TRUE, option = \"magma\", labels = levels) +  # Use viridis magma scale\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 12, face = \"bold\"),\n    legend.text = element_text(size = 10),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5)\n  ) +\n  labs(\n    title = \"Proportioned Water Shortage Levels by Year\",\n    subtitle = \"Center Represents Yearly Mean Shortage Level\",\n    fill = \"Shortage Level\"\n  ) +\n  \n  # Add the white circle in the middle to create the donut hole.\n  annotate(\"point\", x = 0, y = 0, size = 30, color = \"white\") +\n\n  # Add mean shortage level text in the center of each donut.\n  geom_text(data = average_shortage, aes(x = 0, y = 0, label = mean_shortage), \n            color = \"black\", size = 10, fontface = \"bold\", inherit.aes = FALSE)\n\n\n\n\n\n\n\n# Load historical data.\nhistorical_df &lt;- read_csv(here(\"data\", \"historical_production_delivery.csv\"))\n\nRows: 902112 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): pwsid, water_system_name, water_produced_or_delivered, water_type\ndbl  (2): org_id, quantity_acre_feet\ndate (2): start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Get year from start_date.\nhistorical_df &lt;- historical_df %&gt;%\n  mutate(year = year(as.Date(start_date)))\n\n# Identify the #1 producer for each year.\ntop_producer_per_year &lt;- historical_df %&gt;%\n  filter(water_produced_or_delivered == \"water produced\") %&gt;%  # Filter only production.\n  group_by(year, water_system_name) %&gt;%\n  summarise(total_produced = sum(quantity_acre_feet, na.rm = TRUE), .groups = \"drop\") %&gt;%  # Sum per year.\n  arrange(year, desc(total_produced)) %&gt;%  # Sort within each year.\n  group_by(year) %&gt;%\n  slice_head(n = 1)  # Select the #1 producer for each year.\n\n# Get full time series for these top producers.\ntop_producer_trends &lt;- historical_df %&gt;%\n  filter(water_produced_or_delivered == \"water produced\") %&gt;%  # Only produced water\n  semi_join(top_producer_per_year, by = \"water_system_name\") %&gt;%  # Keep only top producers\n  group_by(year, start_date, water_system_name) %&gt;%\n  summarise(total_produced = sum(quantity_acre_feet, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(total_produced_m = total_produced / 1e6)  # Convert to millions\n\n# Sort water systems by total production so the largest is plotted last. \n# Might change later...\ntotal_produced_order &lt;- top_producer_trends %&gt;%\n  group_by(water_system_name) %&gt;%\n  summarise(total_produced = sum(total_produced_m, na.rm = TRUE)) %&gt;%\n  arrange(total_produced) %&gt;%  # Smallest producer first, largest last. (might change)\n  pull(water_system_name)\n\n# To figure out which were the top months by for our suppliers that year.\n# Originally wanted to do top monthly producer plotted with a top yearly\n# producer icon, but was having trouble imagining the calculations and how to\n# get it to plot onto geom_area with our top_producer_trends.\n# to fix if resubmitting. \ntop_producer_month_points &lt;- top_producer_trends %&gt;%\n  left_join(\n    top_producer_per_year %&gt;% \n      select(year, water_system_name),\n    by = c(\"year\", \"water_system_name\")\n  ) %&gt;%\n  filter(!is.na(total_produced))  %&gt;%\n  group_by(year, water_system_name) %&gt;%\n  filter(total_produced == max(total_produced)) %&gt;%\n  slice(1) %&gt;%\n  ungroup()\n\n# Convert to factor with proper order.\ntop_producer_trends$water_system_name &lt;- factor(top_producer_trends$water_system_name, levels = total_produced_order)\n\n# Plot area chart with layered fills.\nggplot(top_producer_trends, aes(x = as.Date(start_date), y = total_produced_m, fill = water_system_name)) +\n  geom_area() +  # Transparent fills, no stacking\n  # Add a special symbol for each top producer's peak month\n  geom_point(\n    data = top_producer_month_points,\n    aes(x = as.Date(start_date), y = total_produced_m),\n    position = position_stack(),\n    shape = 21,       # or 22, 23, etc. 21 is a nice circle with fill\n    size = 3,\n    color = \"black\",  # outline color\n  ) +\n  labs(\n    title = \"Water Production Trends of Yearly #1 Producers\",\n    subtitle = \"Lowest overall producers appear on top, largest producers in the background\",\n    x = \"Time\",\n    y = \"Water Produced (Million Acre-Feet)\",\n    fill = \"Top Producer\",\n    color = \"Top Producer\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nsource_name &lt;- read_csv(here(\"data\", \"source_name.csv\")) |&gt;\n    clean_names()\n\nRows: 10650 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): pwsid, source_facility_id, source_facility_name, source_facility_ac...\ndbl (3): org_id, latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsource_geo &lt;- source_name %&gt;% \n    filter(!is.na(latitude)) %&gt;% \n    filter(!is.na(org_id))\n\n# Load and clean source facility data\nsource_geo &lt;- source_geo %&gt;%\n  select(-c(\"source_facility_name\", \"source_facility_activity_status\", \"source_facility_availability\", \"source_facility_id\"))\n\n# Convert source facility coordinates to spatial format\nsource_geo &lt;- st_as_sf(source_geo, coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4269\") \n\n# Validate CRS of CA_polygon\nst_crs(CA_polygon)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n# Select facility types of interest.\nselected_facility_types &lt;- c(\"well\", \"spring\", \"reservoir\")  # Limit filter for now.\nfiltered_source_geo &lt;- source_geo %&gt;%\n  filter(source_facility_type %in% selected_facility_types)\n\ntmap_mode(\"plot\")  \n\ntmap mode set to plotting\n\n# Build base map.\nbase_map &lt;- \n  tm_shape(CA_polygon) +\n  tm_borders()\n\n# If no data, plot only the base map.\nif (nrow(filtered_source_geo) == 0) {\n  print(base_map)\n} else {\n  base_map +\n    tm_shape(filtered_source_geo) +\n    tm_symbols(\n      col = \"source_facility_type\",  # Color points by facility type.\n      palette = \"Set1\", \n      size = 0.1,\n      title.col = \"Facility Type\"\n    ) +\n    tm_layout(\n      title = \"Water Facilities in California\",\n      title.position = c(\"left\", \"top\"),\n      legend.outside = TRUE,  # Move legend outside the plot.\n      legend.outside.position = \"right\",  # Place legend on the right side.\n      legend.frame = FALSE  # Remove legend border for cleaner look.\n    )\n}"
  }
]